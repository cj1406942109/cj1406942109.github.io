# Speech and Language Processing
> 记录时间：2018-07-07    
> 参考网址：https://web.stanford.edu/~jurafsky/slp3/

## 语言模型
将概率分配给单词序列的模型称为语言模型。


## N-Grams
无论是估计下一个词或整个序列的概率，Ngram模型都是语音和语言处理中最重要的工具之一。

N-Gram（有时也称为N元模型）是自然语言处理中一个非常重要的概念，通常在NLP中，人们基于一定的语料库，可以利用N-Gram来预计或者评估一个句子是否合理。

该模型基于这样一种假设，第`N`个词的出现只与前面`N-1`个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计`N`个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。

对于一个由次序列`w1, w2, w3, ..., wn`组成的句子`S`，那么`P(S) = P(w1w2w3...wn) = P(w1)P(w2|w1)P(w3|w2w1)...P(wn|wn-1...w2w1)`。

但是这种方法存在两个致命的缺陷：
* 一个缺陷是参数空间过大，不可能实用化；
* 另外一个缺陷是数据稀疏严重。

为了解决该问题，我们引入马尔科夫假设：随机过程中各个状态St的概率分布，只与它的前面的有限个状态（一个或几个）相关，即`P(St|S1,S2,S3,...,St-1) = P(St|St-1)`。符合该假设的随机过程被称为马尔科夫过程，也叫马尔科夫链。

此处，如果一个词的出现只依赖于它前面的一个词，那么我们就称之为`bigram`。即`P(S) = P(w1w2w3...wn) = P(w1)p(w2|w1)P(w3|w2)...P(wn|wn-1)`；如果一个词的出现只依赖于它前面的两个词，那么我们就称之为`trigram`。

在实践中用的最多的就是bigram和trigram了，而且效果很不错。高于四元的用的很少，因为训练它需要更庞大的语料，而且数据稀疏严重，时间复杂度高，精度却提高的不多。

我们如何估计这些二元或N-gram概率？ 估计概率的直观方式称为最大似然估计或MLE。

我们通过从标准化语料库中获取计数并对计数进行标准化以使它们位于0和1之间来获得N-gram模型参数的MLE估计值。

例如，为了计算给定前一个词x的单词y的特定双字母概率，我们将计算双字母C（xy）的计数，并通过共享相同第一个单词x的所有双字母的总和进行归一化：`P(wn|wn-1) = C(wn-1wn)/sum(C(wn-1w))`。

此处可以简单理解为，在语料库中，找到wn和wn-1同时出现的次数`C(wn-1wn)`，和wn-1出现的次数`C(wn-1)`，我们就可以得到`P(wn|wn-1) = C(wn-1wn)/C(wn-1)`。

此时，我们就可以计算句子出现的概率啦，如：

已知`P(i|<s>) = 0.25, P(english|want) = 0.0011, P(food|english) = 0.5, P(</s>|food) = 0.68`，则句子*`I want English food`*的概率为:
```
  P(<s> i want english food </s>)
= P(i|<s>)P(want|i)P(english|want)P(food|english)P(</s>|food)
= 0.25 * 0.33 * 0.0011 * 0.5 * 0.68
= 0.000031
```

一个需要注意的问题是稀疏矩阵问题，假设词表中有20000个词，如果是bigram那么可能的N-gram就有400000000个，如果是trigram，那么可能的N-gram就有8000000000000个！那么对于其中的很多词对的组合，在语料库中都没有出现，根据最大似然估计得到的概率将会是0，这会造成很大的麻烦，在算句子的概率时一旦其中的某项为0，那么整个句子的概率就会为0，最后的结果是，我们的模型只能算可怜兮兮的几个句子，而大部分的句子算得的概率是0. 因此，我们要进行数据平滑（data Smoothing），数据平滑的目的有两个：一个是使所有的N-gram概率之和为1，使所有的N-gram概率都不为0。

在实际应用中，我们为了便于数学处理，一般使用`log`形式来表示概率。因为概率小于等于1，我们进行概率乘积计算时，其结果越来越小，可能会导致数值下溢(numerical underflow)。
`P1 * P2 * P3 * P4 = exp(logP1 + logP2 + logP3 + logP4)`。

## 评估语言模型
