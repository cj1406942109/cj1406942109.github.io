# 前馈神经网络
> 记录时间：2018-09-04

前馈神经网络（feedforword neural network, FFNN），也叫多层感知机（multi-layer Perceptron, MLP）或深度神经网络（deep neural network, DNN）。

要解决非线性可分问题，需要使用多层功能神经元，两层感知机可以解决异或问题。

## 结构特点

![前馈神经网络](https://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171259201-1851098378.png)

每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构称为“多层前馈神经网络”。  
输入层与输出层之间的层称为隐层。只要包含隐层，就可称为多层网络。  
其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。即，输入神经元只接收输入，不进行函数处理，隐层和输出层包含功能神经元。

## 误差逆传播（反向传播）算法

神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的阈值。  
多层网络的学习能力比单层感知机强得多，因此需要更强大的学习算法。误差逆传播（error BackPropagation, BP）算法，是迄今最成功的神经网络学习算法。

BP算法不仅可以用于多层前馈神经网络，还可以用于其他类型的神经网络。但通常说“BP网络”，一般指用BP算法训练的多层前馈神经网络。

### 周志华版公式及推导过程

![BP网络及算法中的变量符号](https://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171259654-1114411322.png)

如图，给出一个多层前馈神经网络。其有$d$个输入神经元、$l$个输出神经元、$q$个隐层神经元。其中输出层第$j$个神经元的阈值为$\theta_{j}$，隐含层第$h$个神经元的阈值为$\gamma_{h}$，输入层第$i$个神经元与隐含层第$h$个神经元之间连接的权重为$v_{ih}$，隐含层第$h$个神经元与输出层第$j$个神经元之间连接的权重为$w_{hj}$。

隐含层第$h$个神经元接收到的输入为：$\alpha_{h} = \sum_{i=1}^d v_{ih}x_{i}$  
输出层第$j$个神经元接收到的输入为：$\beta_{j} = \sum_{h=1}^q w_{hj}b_{h}$

其中$b_{h}$为隐含层第$h$个神经元的输出，$b_{h} = f(\alpha_{h} - \gamma_{h})$

对于第$k$个训练例$(\bold{x_{k}}, \bold{y_{k}})$，假定神经网络的输出为$\bold{\hat{y}} = (\hat{y}_{1}^{k}, \hat{y}_{2}^{k}, ..., \hat{y}_{l}^{k})$，即：
$$ \hat{y}_{j}^{k} = f(\beta_{j} - \theta_{j}) $$
网络在$(\bold{x_{k}}, \bold{y_{k}})$上的均方误差为：
$$ E_{k} = \frac{1}{2}\sum_{j=1}^{l}(\hat{y}_{j}^{k} - y_{j}^{k})^2 $$
$$ E_{k} = \frac{1}{2}\sum_{j=1}^{l}\lbrace f\lbrack\sum_{h=1}^{q}w_{hj} f(\sum_{i=1}^{d}v_{ih}x_{i} - \gamma_{h}) - \theta_{j}\rbrack \rbrace^2 $$

图中有$(d+l+1)q + l$个参数需要确定：输入层到隐含层的$d \times q$个权值，隐含层到输出层的$q \times l$个权值，$q$隐含层神经元的阈值，$l$个输出层神经元的阈值。

BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。对误差$E_{k}$给定学习率$\eta$，有
$$ \Delta w_{hj} = -\eta \frac{\partial E_{k}}{\partial w_{hj}} $$

$$ \frac{\partial E_{k}}{\partial w_{hj}} = \frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}}\ \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \frac{\partial \beta_{j}}{\partial w_{hj}}$$
根据定义：
$$\frac{\partial \beta_{j}}{\partial w_{hj}} = b_{h}$$

并且：Sigmoid函数$f(x) = \frac{1}{1+e^{-x}}$有：
$$ f'(x) = \frac{1}{(1+e^{-x})^2}e^{-x} $$
即
$$ f'(x) = f(x)(1 - f(x)) $$

则
$$ \frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}}\ \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} = (\hat{y}_{j}^{k} - y_{j}^{k})f'(\beta_{j} - \theta_{j})
\\
= \hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})$$

因此
$$ \Delta w_{hj} = -\eta \hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})b_{h} $$

同理
$$ \Delta \theta_{j} = \eta \hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k}) $$

继续推导剩下两个梯度：
$$ \Delta v_{ih} = -\eta\frac{\partial E_{k}}{\partial v_{ih}} $$
和
$$ \Delta \gamma_{h} = -\eta\frac{\partial E_{k}}{\partial \gamma_{h}} $$

因为
$$ \frac{\partial E_{k}}{\partial v_{ih}} = \frac{\partial E_{k}}{\partial b_{h}} \frac{\partial b_{h}}{\partial \alpha_{h}} \frac{\partial \alpha_{h}}{\partial v_{ih}} $$
$$ \frac{\partial \alpha_{h}}{\partial v_{ih}} = x_{i} $$
$$ \frac{\partial E_{k}}{\partial b_{h}} \frac{\partial b_{h}}{\partial \alpha_{h}} = \sum_{j=1}^{l} \frac{\partial E_{k}}{\partial \beta_{j}} \frac{\partial \beta_{j}}{\partial b_{h}} f'(\alpha_{h} - \gamma_{h})
\\
= \sum_{j=1}^{l}\hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})w_{hj}b_{h}(1 - b_{h}) $$
所以
$$ \Delta v_{ih} = -\eta \sum_{j=1}^{l}\hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})w_{hj}b_{h}(1 - b_{h})x_{i} $$
同理
$$ \Delta \gamma_{h} = \eta \sum_{j=1}^{l}\hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})w_{hj}b_{h}(1 - b_{h}) $$

综上，令
$$ g_{j} = \hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k}) $$
$$ e_{h} = b_{h}(1 - b_{h})\sum_{j=1}^{l}w_{hj}g_{j} $$

得到：
$$ \Delta w_{hj} =  -\eta g_{j} b_{h}$$
$$ \Delta \theta_{j} =  \eta g_{j}$$
$$ \Delta v_{ih} = -\eta e_{h}x_{i}$$
$$ \Delta \gamma_{h} =  \eta e_{h}$$

BP算法的流程如下：

![BP算法流程](https://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171305935-1361097696.png)

对于每个训练样例，BP算法执行以下操作：

1. 先将输入示例提供给输入层神经元
2. 然后逐层将信号前传，直到产生输出层的结果
3. 然后计算输出层的误差
4. 再将误差逆向传播至隐层神经元
5. 最后根据隐层神经元的误差来对连接权和阈值进行调整
6. 迭代以上过程、直到达到停止条件（如：训练误差达到很小的值）

以上是标准BP算法的训练过程，每次仅针对一个训练样例$(x_{k}, y_{k})$更新连接权和阈值，更新规则是基于单个的$E_{k}$推导得到。

如果类似地推导出基于累计误差$E = \frac{1}{m}\sum_{k=1}^{m}E_{k}$最小化的更新规则，就得到累积误差逆传播算法。

一般来说，标准BP算法迭代次数更多，累积BP算法参数更新频率低得多；但在很多任务中，累积误差下降到一定程度之后，进一步下降会很缓慢，这是标准BP算法往往会更快获得较好解，尤其在训练集$D$非常大时更明显。

## 实际应用中要注意的问题

1. 隐含层神经元个数  
    经证明，只要一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂度的连续函数。

    但是，如何设置隐层神经元的个数仍是个未解决的问题。实际应用中通常用“试错法”调整。
2. BP神经网络过拟合  
   由于BP神经网络强大的表示能力，经常会遭遇过拟合。有两种策略常用来缓和BP网络的过拟合  
   - 早停：将数据分成训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差。若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值
   - 正则化：在误差目标函数中增加一个用于描述网络复杂度的部分，如连接权与阈值的平方和。令$E_{k}$表示第$k$个训练样例上的误差，$w_{i}$表示连接权和阈值，则误差目标函数变为
    $$ E = \lambda \frac{1}{m} \sum_{k=1}^{m}E_{k}+(1-\lambda)\sum_{i}w_{i}^2 $$
    其中，$\lambda \in (0, 1)$用于对经验误差与网络复杂度这两项进行这种，常通过交叉验证法来估计。

## 全局最小与局部极小

基于梯度的搜索是使用最为广泛的参数寻优方法。在此类方法中，从某些初始解出发，迭代寻找最有参数值。每次迭代，先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。

由于负梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。

若误差函数在当前点的梯度为零，则已达到局部极小。如果误差函数仅有一个局部极小，那么该局部极小就是全局最小；如果误差函数有多个局部最下，则不能保证找到的解是全局最小。

现实任务中，常用以下策略来试图跳出局部极小：

- 以多组不同参数值初始化多个神经网络，按标准方法训练之后，取其中误差最小的解作为最终参数。（相当于从多个不同的初始点进行搜索，这样就可能陷入不同的局部极小，从中选取有可能获得更接近全局最小的结果。）
- 使用“模拟退火”技术。  
  模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于跳出局部极小。在每次迭代过程中，接受“次优解”的概率要随着时间的推移而逐渐降低，从而保证算法稳定。
- 使用随机梯度下降。
  随机梯度下降法在计算梯度时加入了随机因素，于是，即便陷入了局部极小点，计算出的梯度仍可能不为零，就有机会跳出局部极小继续搜索。
- 遗传算法
