# 前馈神经网络
> 记录时间：2018-09-04

前馈神经网络（feedforword neural network, FFNN），也叫多层感知机（multi-layer Perceptron, MLP）或深度神经网络（deep neural network, DNN）。

要解决非线性可分问题，需要使用多层功能神经元，两层感知机可以解决异或问题。

## 结构特点

![前馈神经网络](https://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171259201-1851098378.png)

每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构称为“多层前馈神经网络”。  
输入层与输出层之间的层称为隐层。只要包含隐层，就可称为多层网络。  
其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。即，输入神经元只接收输入，不进行函数处理，隐层和输出层包含功能神经元。

## 误差逆传播（反向传播）算法

神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的阈值。  
多层网络的学习能力比单层感知机强得多，因此需要更强大的学习算法。误差逆传播（error BackPropagation, BP）算法，是迄今最成功的神经网络学习算法。

BP算法不仅可以用于多层前馈神经网络，还可以用于其他类型的神经网络。但通常说“BP网络”，一般指用BP算法训练的多层前馈神经网络。

### 周志华版公式及推导过程

![BP网络及算法中的变量符号](https://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171259654-1114411322.png)

如图，给出一个多层前馈神经网络。其有$d$个输入神经元、$l$个输出神经元、$q$个隐层神经元。其中输出层第$j$个神经元的阈值为$\theta_{j}$，隐含层第$h$个神经元的阈值为$\gamma_{h}$，输入层第$i$个神经元与隐含层第$h$个神经元之间连接的权重为$v_{ih}$，隐含层第$h$个神经元与输出层第$j$个神经元之间连接的权重为$w_{hj}$。

隐含层第$h$个神经元接收到的输入为：$\alpha_{h} = \sum_{i=1}^d v_{ih}x_{i}$  
输出层第$j$个神经元接收到的输入为：$\beta_{j} = \sum_{h=1}^q w_{hj}b_{h}$

其中$b_{h}$为隐含层第$h$个神经元的输出，$b_{h} = f(\alpha_{h} - \gamma_{h})$

对于第$k$个训练例$(\bold{x_{k}}, \bold{y_{k}})$，假定神经网络的输出为$\bold{\hat{y}} = (\hat{y}_{1}^{k}, \hat{y}_{2}^{k}, ..., \hat{y}_{l}^{k})$，即：
$$ \hat{y}_{j}^{k} = f(\beta_{j} - \theta_{j}) $$
网络在$(\bold{x_{k}}, \bold{y_{k}})$上的均方误差为：
$$ E_{k} = \frac{1}{2}\sum_{j=1}^{l}(\hat{y}_{j}^{k} - y_{j}^{k})^2 $$
$$ E_{k} = \frac{1}{2}\sum_{j=1}^{l}\lbrace f\lbrack\sum_{h=1}^{q}w_{hj} f(\sum_{i=1}^{d}v_{ih}x_{i} - \gamma_{h}) - \theta_{j}\rbrack \rbrace^2 $$

图中有$(d+l+1)q + l$个参数需要确定：输入层到隐含层的$d \times q$个权值，隐含层到输出层的$q \times l$个权值，$q$隐含层神经元的阈值，$l$个输出层神经元的阈值。

BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。对误差$E_{k}$给定学习率$\eta$，有
$$ \Delta w_{hj} = -\eta \frac{\partial E_{k}}{\partial w_{hj}} $$

$$ \frac{\partial E_{k}}{\partial w_{hj}} = \frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}}\ \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \frac{\partial \beta_{j}}{\partial w_{hj}}$$
根据定义：
$$\frac{\partial \beta_{j}}{\partial w_{hj}} = b_{h}$$

并且：Sigmoid函数$f(x) = \frac{1}{1+e^{-x}}$有：
$$ f'(x) = \frac{1}{(1+e^{-x})^2}e^{-x} $$
即
$$ f'(x) = f(x)(1 - f(x)) $$

则
$$ \frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}}\ \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} = (\hat{y}_{j}^{k} - y_{j}^{k})f'(\beta_{j} - \theta_{j})
\\
= \hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})$$

因此
$$ \Delta w_{hj} = -\eta \hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})b_{h} $$

同理
$$ \Delta \theta_{j} = \eta \hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k}) $$

继续推导剩下两个梯度：
$$ \Delta v_{ih} = -\eta\frac{\partial E_{k}}{\partial v_{ih}} $$
和
$$ \Delta \gamma_{h} = -\eta\frac{\partial E_{k}}{\partial \gamma_{h}} $$

因为
$$ \frac{\partial E_{k}}{\partial v_{ih}} = \frac{\partial E_{k}}{\partial b_{h}} \frac{\partial b_{h}}{\partial \alpha_{h}} \frac{\partial \alpha_{h}}{\partial v_{ih}} $$
$$ \frac{\partial \alpha_{h}}{\partial v_{ih}} = x_{i} $$
$$ \frac{\partial E_{k}}{\partial b_{h}} \frac{\partial b_{h}}{\partial \alpha_{h}} = \sum_{j=1}^{l} \frac{\partial E_{k}}{\partial \beta_{j}} \frac{\partial \beta_{j}}{\partial b_{h}} f'(\alpha_{h} - \gamma_{h})
\\
= \sum_{j=1}^{l}\hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})w_{hj}b_{h}(1 - b_{h}) $$
所以
$$ \Delta v_{ih} = -\eta \sum_{j=1}^{l}\hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})w_{hj}b_{h}(1 - b_{h})x_{i} $$
同理
$$ \Delta \gamma_{h} = \eta \sum_{j=1}^{l}\hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k})w_{hj}b_{h}(1 - b_{h}) $$

综上，令
$$ g_{j} = \hat{y}_{j}^{k}(1 - \hat{y}_{j}^{k})(\hat{y}_{j}^{k} - y_{j}^{k}) $$
$$ e_{h} = b_{h}(1 - b_{h})\sum_{j=1}^{l}w_{hj}g_{j} $$

得到：
$$ \Delta w_{hj} =  -\eta g_{j} b_{h}$$
$$ \Delta \theta_{j} =  \eta g_{j}$$
$$ \Delta v_{ih} = -\eta e_{h}x_{i}$$
$$ \Delta \gamma_{h} =  \eta e_{h}$$

BP算法的流程如下：

![BP算法流程](https://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171305935-1361097696.png)

对于每个训练样例，BP算法执行以下操作：

1. 先将输入示例提供给输入层神经元
2. 然后逐层将信号前传，直到产生输出层的结果
3. 然后计算输出层的误差
4. 再将误差逆向传播至隐层神经元
5. 最后根据隐层神经元的误差来对连接权和阈值进行调整
6. 迭代以上过程、直到达到停止条件（如：训练误差达到很小的值）

以上是标准BP算法的训练过程，每次仅针对一个训练样例$(x_{k}, y_{k})$更新连接权和阈值，更新规则是基于单个的$E_{k}$推导得到。

如果类似地推导出基于累计误差$E = \frac{1}{m}\sum_{k=1}^{m}E_{k}$最小化的更新规则，就得到累积误差逆传播算法。

一般来说，标准BP算法迭代次数更多，累积BP算法参数更新频率低得多；但在很多任务中，累积误差下降到一定程度之后，进一步下降会很缓慢，这是标准BP算法往往会更快获得较好解，尤其在训练集$D$非常大时更明显。

## 实际应用中要注意的问题

1. 隐含层神经元个数  
    经证明，只要一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂度的连续函数。

    但是，如何设置隐层神经元的个数仍是个未解决的问题。实际应用中通常用“试错法”调整。
2. BP神经网络过拟合  
   由于BP神经网络强大的表示能力，经常会遭遇过拟合。有两种策略常用来缓和BP网络的过拟合  
   - 早停：将数据分成训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差。若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值
   - 正则化：在误差目标函数中增加一个用于描述网络复杂度的部分，如连接权与阈值的平方和。令$E_{k}$表示第$k$个训练样例上的误差，$w_{i}$表示连接权和阈值，则误差目标函数变为
    $$ E = \lambda \frac{1}{m} \sum_{k=1}^{m}E_{k}+(1-\lambda)\sum_{i}w_{i}^2 $$
    其中，$\lambda \in (0, 1)$用于对经验误差与网络复杂度这两项进行这种，常通过交叉验证法来估计。

## 全局最小与局部极小

基于梯度的搜索是使用最为广泛的参数寻优方法。在此类方法中，从某些初始解出发，迭代寻找最有参数值。每次迭代，先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。

由于负梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。

若误差函数在当前点的梯度为零，则已达到局部极小。如果误差函数仅有一个局部极小，那么该局部极小就是全局最小；如果误差函数有多个局部最下，则不能保证找到的解是全局最小。

现实任务中，常用以下策略来试图跳出局部极小：

- 以多组不同参数值初始化多个神经网络，按标准方法训练之后，取其中误差最小的解作为最终参数。（相当于从多个不同的初始点进行搜索，这样就可能陷入不同的局部极小，从中选取有可能获得更接近全局最小的结果。）
- 使用“模拟退火”技术。  
  模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于跳出局部极小。在每次迭代过程中，接受“次优解”的概率要随着时间的推移而逐渐降低，从而保证算法稳定。
- 使用随机梯度下降。
  随机梯度下降法在计算梯度时加入了随机因素，于是，即便陷入了局部极小点，计算出的梯度仍可能不为零，就有机会跳出局部极小继续搜索。
- 遗传算法

## 神经网络python实现

参考博客[Implementing a Neural Network from Scratch in Python](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)

### 生成数据集

使用`scikit-learn`的数据集生成器

```py
# coding: utf-8
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt

np.random.seed(0)
X, y = datasets.make_moons(200, noise=0.20) # moon shape data
plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)
plt.show()
```

![数据集图示](http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-dataset.png)

生成的数据集有两类，分别用红色和蓝色的点表示。

我们的目标是训练机器学习分类器，能够对给定样例预测正确的类。注意这里的数据不是线性可分的，不能画一条直线来将两个类分开。这意味着像逻辑回归之类的线性分类器不能很好地适应数据。

### 逻辑回归

我们先使用逻辑回归看看分类的结果

```py
from sklearn import linear_model

# 训练逻辑回归分类器
clf = linear_model.LogisticRegressionCV()
clf.fit(X, y)

# 绘出判定边界
def plot_decision_boundary(pred_func):

    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = 0.01

    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)

plot_decision_boundary(lambda x: clf.predict(x))
plt.title('Logistic Regression')
plt.show()
```

![逻辑回归分类器](http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-lr-decision-boundary.png)

如图示，使用直线尽可能地分类了数据，但是无法捕获数据的“月牙”形状。

### 训练神经网络

我们来构建一个三层的神经网络，包含一输入层、一隐层和一输出层。
输入层的结点数由数据的维度确定，这里是2。
输出的层的结点数由数据的类别确定，这里也是2。

隐含层的结点数由我们选择， 结点数越多，要适应的函数就越复杂，参数越多，在学习网络的参数和做预测时就要更多的计算资源。并且，参数越多，越有可能过拟合数据。

隐层结点数量的选择常常依赖于特定的问题，更多是一门艺术而不是一门科学。

我们还需要为隐层选择一个激活函数，激活函数将隐层的输入转换为输出。一个非线性的激活函数可以适应非线性假设。

常用的激活函数有S型（Sigmoid）函数、双曲正切（Tanh）函数或线性整流（ReLU）函数。这些函数的好处之一是其导数可以由其本身的函数值计算得到，如$tanh(x) = 1 - tanh(x)^2$。

这里隐层的激活函数选择双曲正切函数。输出层的激活函数选择`Softmax`函数。

网络使用正向传播进行预测，$x$是2维输入，我们根据以下公式计算预测值$\hat{y}$：
$$ z_{1} = xW_{1} + b_{1} $$
$$ a_{1} = tanh(z_{1}) $$
$$ z_{2} = a1W_{2} + b_{2} $$
$$ a_{2} = \hat{y} = softmax(z_{2}) $$

其中$W_{1}, b_{1}, W_{2}, b_{2}$是网络的参数，需要从训练数据中学习得到。
如果隐层结点数为$500$，则$W_{1} \in \mathbb{R}^{2 \times 500}$，$b_{1} \in \mathbb{R}^{500}$，$W_{2} \in \mathbb{R}^{500 \times 2}$，$b_{2} \in \mathbb{R}^{500}$。

学习参数意味着找到能够最小化训练数据错误的参数$(W_{1}, b_{1}, W_{2}, b_{2})$。

我们把衡量错误地函数叫做损失函数。softmax输出的常见选择是分类交叉熵损失（也叫负对数似然）。如果有$N$个训练样例，$C$个类，则预测$\hat{y}$相对于正确的标签$y$的损失为：
$$ L(y, \hat{y}) = - \frac{1}{N} \sum_{n \in N} \sum_{i \in C} y_{n,i}\log\hat{y}_{n,i}$$

我们使用梯度下降法来寻找最小值。

梯度下降法需要损失函数相对于参数的梯度：$\frac{\partial L}{\partial W_{1}}$，$\frac{\partial L}{\partial b_{1}}$，$\frac{\partial L}{\partial W_{2}}$，$\frac{\partial L}{\partial b_{2}}$

为了计算梯度，我们使用反向传播算法。
$$ \delta_{3} = \hat{y} - y $$
$$ \delta_{2} = (1 - tanh^2z_{1}) \circ \delta_{3}W_{2}^T $$
$$ \frac{\partial L}{\partial W_{1}} = a_{1}^T\delta_{3} $$
$$ \frac{\partial L}{\partial b_{1}} = \delta_{3} $$
$$ \frac{\partial L}{\partial W_{2}} = x^T\delta_{2} $$
$$ \frac{\partial L}{\partial b_{2}} = \delta_{2} $$

```py
# 训练集size
num_examples = len(X)
# 输入层维度
nn_input_dim = 2
# 输出层维度
nn_output_dim = 2

# 梯度下降法参数
epsilon = 0.01 # 学习率
reg_lambda = 0.01 # 正则化强度

# 计算数据集的总损失
def calculate_loss(model):
  W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']

  z1 = X.dot(W1) + b1
  a1 = np.tanh(z1)
  z2 = a1.dot(W2) + b2
  exp_scores = np.exp(z2)

  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

  # 计算损失
  corect_logprobs = -np.log(probs[range(num_examples), y])
  data_loss = np.sum(corect_logprobs)

  # 添加正则项
  data_loss += reg_lambda / 2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))

  return 1./num_examples * data_loss

# 预测输出
def predict(model, X)
  W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
  z1 = X.dot(W1) + b1
  a1 = np.tanh(z1)
  z2 = a1.dot(W2) + b2
  exp_scores = np.exp(z2)

  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

  return np.argmax(probs, axis=1)

# 训练网络
# This function learns parameters for the neural network and returns the model.
# - nn_hdim: Number of nodes in the hidden layer
# - num_passes: Number of passes through the training data for gradient descent
# - print_loss: If True, print the loss every 1000 iterations
def build_model(nn_hdim, num_passes=20000, print_loss=False):
  # Initialize the parameters to random values. We need to learn these.
  np.random.seed(0)
  W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)
  b1 = np.zeros((1, nn_hdim))
  W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)
  b2 = np.zeros((1, nn_output_dim))

  # This is what we return at the end
  model = {}

  # Gradient descent. For each batch...
  for i in xrange(0, num_passes):

    # Forward propagation
    z1 = X.dot(W1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(W2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

    # Backpropagation
    delta3 = probs
    delta3[range(num_examples), y] -= 1
    dW2 = (a1.T).dot(delta3)
    db2 = np.sum(delta3, axis=0, keepdims=True)
    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))
    dW1 = np.dot(X.T, delta2)
    db1 = np.sum(delta2, axis=0)

    # Add regularization terms (b1 and b2 don't have regularization terms)
    dW2 += reg_lambda * W2
    dW1 += reg_lambda * W1

    # Gradient descent parameter update
    W1 += -epsilon * dW1
    b1 += -epsilon * db1
    W2 += -epsilon * dW2
    b2 += -epsilon * db2

    # Assign new parameters to the model
    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}

    # Optionally print the loss.
    # This is expensive because it uses the whole dataset, so we don't want to do it too often.
    if print_loss and i % 1000 == 0:
      print "Loss after iteration %i: %f" %(i, calculate_loss(model))

  return model

# 创建一个隐层结点数为3的模型
model = build_model(3, print_loss=True)

# 绘制判定边界
plot_decision_boundary(lambda x: predict(model, x))
plt.title('Decision Boundary for hidden layer size 3')
plt.show()
```

![隐层结点数为3的模型](http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-h3.png)

### 改变隐层结点数

```py
plt.figure(figsize=(16, 32))
hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]
for i, nn_hdim in enumerate(hidden_layer_dimensions):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer size %d' % nn_hdim)
    model = build_model(nn_hdim)
    plot_decision_boundary(lambda x: predict(model, x))
plt.show()
```

![不同隐层结点数对应结果](http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-hidden-layer-varying.png)
